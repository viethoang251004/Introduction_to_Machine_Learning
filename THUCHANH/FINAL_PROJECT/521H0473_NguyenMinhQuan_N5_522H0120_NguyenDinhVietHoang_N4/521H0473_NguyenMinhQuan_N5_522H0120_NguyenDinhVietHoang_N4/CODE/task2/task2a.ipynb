{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Câu a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('winequality-red.csv')\n",
    "\n",
    "# Prepare the data\n",
    "X = data.drop('quality', axis=1)\n",
    "y = data['quality']\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to the appropriate format for CNNs (add a third dimension)\n",
    "X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 20.6892 - mae: 4.2269 - val_loss: 3.6495 - val_mae: 1.5424\n",
      "Epoch 2/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7143 - mae: 1.5599 - val_loss: 2.3107 - val_mae: 1.2285\n",
      "Epoch 3/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7496 - mae: 1.3180 - val_loss: 1.7613 - val_mae: 1.0639\n",
      "Epoch 4/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2544 - mae: 1.1552 - val_loss: 1.3612 - val_mae: 0.9227\n",
      "Epoch 5/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8892 - mae: 1.0979 - val_loss: 1.0463 - val_mae: 0.8076\n",
      "Epoch 6/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5979 - mae: 1.0158 - val_loss: 0.9641 - val_mae: 0.7780\n",
      "Epoch 7/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4379 - mae: 0.9470 - val_loss: 0.6721 - val_mae: 0.6537\n",
      "Epoch 8/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2351 - mae: 0.8924 - val_loss: 0.5985 - val_mae: 0.6130\n",
      "Epoch 9/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2879 - mae: 0.8804 - val_loss: 0.5484 - val_mae: 0.5892\n",
      "Epoch 10/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0836 - mae: 0.8327 - val_loss: 0.5169 - val_mae: 0.5692\n",
      "Epoch 11/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0869 - mae: 0.8226 - val_loss: 0.5025 - val_mae: 0.5596\n",
      "Epoch 12/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0602 - mae: 0.8139 - val_loss: 0.4826 - val_mae: 0.5692\n",
      "Epoch 13/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0265 - mae: 0.8108 - val_loss: 0.4732 - val_mae: 0.5597\n",
      "Epoch 14/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1154 - mae: 0.8477 - val_loss: 0.4984 - val_mae: 0.5560\n",
      "Epoch 15/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0634 - mae: 0.8173 - val_loss: 0.6056 - val_mae: 0.6074\n",
      "Epoch 16/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9397 - mae: 0.7618 - val_loss: 0.4785 - val_mae: 0.5521\n",
      "Epoch 17/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9743 - mae: 0.7843 - val_loss: 0.4409 - val_mae: 0.5359\n",
      "Epoch 18/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9837 - mae: 0.7828 - val_loss: 0.5507 - val_mae: 0.5779\n",
      "Epoch 19/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9445 - mae: 0.7686 - val_loss: 0.4467 - val_mae: 0.5419\n",
      "Epoch 20/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1223 - mae: 0.8411 - val_loss: 0.4884 - val_mae: 0.5555\n",
      "Epoch 21/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9425 - mae: 0.7789 - val_loss: 0.5461 - val_mae: 0.5726\n",
      "Epoch 22/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9480 - mae: 0.7649 - val_loss: 0.4468 - val_mae: 0.5389\n",
      "Epoch 23/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9972 - mae: 0.7909 - val_loss: 0.4406 - val_mae: 0.5392\n",
      "Epoch 24/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9657 - mae: 0.7831 - val_loss: 0.4884 - val_mae: 0.5525\n",
      "Epoch 25/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0126 - mae: 0.8180 - val_loss: 0.5335 - val_mae: 0.5741\n",
      "Epoch 26/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9706 - mae: 0.7802 - val_loss: 0.4342 - val_mae: 0.5375\n",
      "Epoch 27/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8945 - mae: 0.7540 - val_loss: 0.4309 - val_mae: 0.5438\n",
      "Epoch 28/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9546 - mae: 0.7831 - val_loss: 0.4434 - val_mae: 0.5348\n",
      "Epoch 29/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9689 - mae: 0.7793 - val_loss: 0.4462 - val_mae: 0.5330\n",
      "Epoch 30/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9543 - mae: 0.7722 - val_loss: 0.4446 - val_mae: 0.5379\n",
      "Epoch 31/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9557 - mae: 0.7696 - val_loss: 0.4744 - val_mae: 0.5418\n",
      "Epoch 32/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8870 - mae: 0.7473 - val_loss: 0.4128 - val_mae: 0.5297\n",
      "Epoch 33/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9217 - mae: 0.7718 - val_loss: 0.4456 - val_mae: 0.5361\n",
      "Epoch 34/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8564 - mae: 0.7336 - val_loss: 0.5185 - val_mae: 0.5706\n",
      "Epoch 35/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8752 - mae: 0.7415 - val_loss: 0.4408 - val_mae: 0.5298\n",
      "Epoch 36/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8288 - mae: 0.7362 - val_loss: 0.4834 - val_mae: 0.5462\n",
      "Epoch 37/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8912 - mae: 0.7551 - val_loss: 0.4179 - val_mae: 0.5296\n",
      "Epoch 38/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8763 - mae: 0.7351 - val_loss: 0.4232 - val_mae: 0.5350\n",
      "Epoch 39/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9111 - mae: 0.7412 - val_loss: 0.4288 - val_mae: 0.5223\n",
      "Epoch 40/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9321 - mae: 0.7594 - val_loss: 0.4192 - val_mae: 0.5236\n",
      "Epoch 41/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9102 - mae: 0.7556 - val_loss: 0.4414 - val_mae: 0.5395\n",
      "Epoch 42/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8193 - mae: 0.7204 - val_loss: 0.4750 - val_mae: 0.5503\n",
      "Epoch 43/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8692 - mae: 0.7430 - val_loss: 0.4274 - val_mae: 0.5252\n",
      "Epoch 44/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8724 - mae: 0.7261 - val_loss: 0.4418 - val_mae: 0.5439\n",
      "Epoch 45/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9230 - mae: 0.7675 - val_loss: 0.4191 - val_mae: 0.5197\n",
      "Epoch 46/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7798 - mae: 0.6907 - val_loss: 0.4143 - val_mae: 0.5244\n",
      "Epoch 47/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9020 - mae: 0.7562 - val_loss: 0.4093 - val_mae: 0.5170\n",
      "Epoch 48/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8204 - mae: 0.7131 - val_loss: 0.4065 - val_mae: 0.5165\n",
      "Epoch 49/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7706 - mae: 0.6964 - val_loss: 0.4407 - val_mae: 0.5282\n",
      "Epoch 50/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8518 - mae: 0.7446 - val_loss: 0.4104 - val_mae: 0.5170\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "# Build the CNN model\n",
    "model_cnn = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='linear')  # Use 'linear' for regression tasks\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the CNN model\n",
    "history_cnn = model_cnn.fit(X_train_cnn, y_train, epochs=50, validation_data=(X_test_cnn, y_test), verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "# 1. Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1))\n",
    "# Conv1D: Đây là một lớp tích chập 1 chiều.\n",
    "# 32: Điều này xác định số lượng bộ lọc (hoặc nhân) trong lớp tích chập. Các bộ lọc được sử dụng để phát hiện các đặc trưng khác nhau trong dữ liệu đầu vào.\n",
    "# kernel_size=3: Đây là kích thước của nhân tích chập. Kích thước nhân là 3 nghĩa là lớp sẽ xem xét ba phần tử đầu vào cùng một lúc.\n",
    "# activation='relu': Điều này xác định hàm kích hoạt được sử dụng cho lớp. ReLU (Rectified Linear Unit) giới thiệu tính phi tuyến vào mô hình, giúp mô hình học các mẫu phức tạp hơn.\n",
    "# input_shape=(X_train_cnn.shape[1], 1): Điều này định nghĩa hình dạng của dữ liệu đầu vào. X_train_cnn.shape[1] là số lượng đặc trưng (hoặc các bước thời gian trong dữ liệu tuần tự), và 1 chỉ ra rằng mỗi đặc trưng là một giá trị đơn (không phải đa kênh).\n",
    "# 2. MaxPooling1D(pool_size=2)\n",
    "# MaxPooling1D: Đây là một lớp gộp tối đa 1 chiều.\n",
    "# pool_size=2: Điều này xác định kích thước của cửa sổ gộp. Gộp giúp giảm độ phức tạp của dữ liệu bằng cách lấy giá trị lớn nhất trong một cửa sổ có kích thước 2, hiệu quả là giảm số lượng đặc trưng xuống một nửa. Hoạt động này giúp giảm thiểu tính toán và kiểm soát overfitting.\n",
    "# 3. Conv1D(64, kernel_size=3, activation='relu')\n",
    "# Conv1D: Một lớp tích chập 1 chiều khác.\n",
    "# 64: Điều này xác định số lượng bộ lọc. Nhiều bộ lọc hơn cho phép mô hình học nhiều đặc trưng hơn.\n",
    "# kernel_size=3: Kích thước của nhân tích chập vẫn là 3.\n",
    "# activation='relu': Sử dụng hàm kích hoạt ReLU để giới thiệu tính phi tuyến.\n",
    "# 4. MaxPooling1D(pool_size=2)\n",
    "# MaxPooling1D: Một lớp gộp tối đa khác.\n",
    "# pool_size=2: Kích thước cửa sổ gộp vẫn là 2, tiếp tục giảm độ phức tạp của dữ liệu.\n",
    "# 5. Flatten()\n",
    "# Flatten: Lớp này làm phẳng dữ liệu đầu vào, chuyển đổi dữ liệu 2 chiều (từ các lớp tích chập) thành một vector 1 chiều. Điều này cần thiết trước khi đưa dữ liệu vào các lớp dense (kết nối đầy đủ).\n",
    "# 6. Dense(128, activation='relu')\n",
    "# Dense: Đây là một lớp kết nối đầy đủ (dense).\n",
    "# 128: Điều này xác định số lượng neuron trong lớp. Các lớp kết nối đầy đủ giúp học các đặc trưng cấp cao hơn từ dữ liệu đầu vào.\n",
    "# activation='relu': Sử dụng hàm kích hoạt ReLU để giới thiệu tính phi tuyến.\n",
    "# 7. Dropout(0.5)\n",
    "# Dropout: Đây là một kỹ thuật regularization được sử dụng để ngăn ngừa overfitting.\n",
    "# 0.5: Điều này xác định tỷ lệ dropout, nghĩa là 50% các neuron sẽ được đặt ngẫu nhiên thành zero trong mỗi bước đào tạo. Dropout giúp làm cho mô hình trở nên mạnh mẽ hơn bằng cách ngăn chặn nó quá phụ thuộc vào bất kỳ neuron cụ thể nào.\n",
    "# 8. Dense(1, activation='linear')\n",
    "# Dense: Đây là lớp đầu ra.\n",
    "# 1: Điều này xác định số lượng neuron đầu ra, trong trường hợp này là 1, vì chúng ta đang thực hiện một nhiệm vụ hồi quy nơi mà chúng ta dự đoán một giá trị liên tục duy nhất.\n",
    "# activation='linear': Sử dụng hàm kích hoạt tuyến tính cho các nhiệm vụ hồi quy. Điều này có nghĩa là đầu ra sẽ là một kết hợp tuyến tính của các đầu vào.\n",
    "# Biên dịch và Đào tạo\n",
    "# compile(optimizer='adam', loss='mean_squared_error', metrics=['mae']):\n",
    "\n",
    "# optimizer='adam': Adam là một thuật toán tối ưu hóa tốc độ học tập thích nghi phổ biến vì tính hiệu quả và hiệu suất tốt.\n",
    "# loss='mean_squared_error': Mean Squared Error (MSE) được sử dụng làm hàm mất mát cho các nhiệm vụ hồi quy. Nó đo lường trung bình của các bình phương của các lỗi.\n",
    "# metrics=['mae']: Mean Absolute Error (MAE) được sử dụng làm chỉ số bổ sung để đánh giá hiệu suất của mô hình.\n",
    "# fit(X_train_cnn, y_train, epochs=50, validation_data=(X_test_cnn, y_test), verbose=1):\n",
    "\n",
    "# X_train_cnn, y_train: Dữ liệu đào tạo và nhãn tương ứng.\n",
    "# epochs=50: Số lượng epochs (lượt đào tạo đầy đủ trên dữ liệu đào tạo) để đào tạo mô hình.\n",
    "# validation_data=(X_test_cnn, y_test): Dữ liệu xác thực và nhãn tương ứng, được sử dụng để đánh giá hiệu suất của mô hình trên dữ liệu chưa thấy trong quá trình đào tạo.\n",
    "# verbose=1: Điều này kiểm soát độ chi tiết của đầu ra. 1 nghĩa là tiến trình đào tạo sẽ được hiển thị."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 22.0034 - mae: 4.4807 - val_loss: 3.9111 - val_mae: 1.6026\n",
      "Epoch 2/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2888 - mae: 1.5873 - val_loss: 2.5112 - val_mae: 1.2830\n",
      "Epoch 3/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.9180 - mae: 1.3754 - val_loss: 1.9237 - val_mae: 1.1009\n",
      "Epoch 4/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5871 - mae: 1.2734 - val_loss: 1.5955 - val_mae: 1.0027\n",
      "Epoch 5/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3169 - mae: 1.2077 - val_loss: 1.4425 - val_mae: 0.9611\n",
      "Epoch 6/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0593 - mae: 1.1446 - val_loss: 1.2183 - val_mae: 0.8764\n",
      "Epoch 7/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8793 - mae: 1.0890 - val_loss: 1.1274 - val_mae: 0.8450\n",
      "Epoch 8/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7464 - mae: 1.0493 - val_loss: 1.0300 - val_mae: 0.8036\n",
      "Epoch 9/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5981 - mae: 0.9945 - val_loss: 0.9097 - val_mae: 0.7586\n",
      "Epoch 10/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5382 - mae: 0.9883 - val_loss: 0.8663 - val_mae: 0.7425\n",
      "Epoch 11/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4283 - mae: 0.9441 - val_loss: 0.7720 - val_mae: 0.7007\n",
      "Epoch 12/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3164 - mae: 0.9098 - val_loss: 0.7230 - val_mae: 0.6795\n",
      "Epoch 13/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2934 - mae: 0.8927 - val_loss: 0.7132 - val_mae: 0.6735\n",
      "Epoch 14/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1600 - mae: 0.8562 - val_loss: 0.6273 - val_mae: 0.6298\n",
      "Epoch 15/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0650 - mae: 0.8316 - val_loss: 0.7019 - val_mae: 0.6650\n",
      "Epoch 16/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0672 - mae: 0.8130 - val_loss: 0.5755 - val_mae: 0.6051\n",
      "Epoch 17/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2472 - mae: 0.8729 - val_loss: 0.5473 - val_mae: 0.5840\n",
      "Epoch 18/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0073 - mae: 0.7920 - val_loss: 0.5613 - val_mae: 0.5911\n",
      "Epoch 19/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0018 - mae: 0.7794 - val_loss: 0.5063 - val_mae: 0.5650\n",
      "Epoch 20/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9964 - mae: 0.7758 - val_loss: 0.5922 - val_mae: 0.6072\n",
      "Epoch 21/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0396 - mae: 0.8023 - val_loss: 0.4920 - val_mae: 0.5590\n",
      "Epoch 22/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9576 - mae: 0.7758 - val_loss: 0.4595 - val_mae: 0.5438\n",
      "Epoch 23/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9412 - mae: 0.7698 - val_loss: 0.4666 - val_mae: 0.5488\n",
      "Epoch 24/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9671 - mae: 0.7702 - val_loss: 0.4518 - val_mae: 0.5334\n",
      "Epoch 25/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8302 - mae: 0.7190 - val_loss: 0.4700 - val_mae: 0.5453\n",
      "Epoch 26/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9529 - mae: 0.7730 - val_loss: 0.4431 - val_mae: 0.5244\n",
      "Epoch 27/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8835 - mae: 0.7474 - val_loss: 0.4286 - val_mae: 0.5183\n",
      "Epoch 28/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7846 - mae: 0.6901 - val_loss: 0.4344 - val_mae: 0.5240\n",
      "Epoch 29/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8857 - mae: 0.7539 - val_loss: 0.4314 - val_mae: 0.5137\n",
      "Epoch 30/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9080 - mae: 0.7619 - val_loss: 0.4285 - val_mae: 0.5149\n",
      "Epoch 31/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8717 - mae: 0.7446 - val_loss: 0.4065 - val_mae: 0.5036\n",
      "Epoch 32/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8253 - mae: 0.7211 - val_loss: 0.4148 - val_mae: 0.5079\n",
      "Epoch 33/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8511 - mae: 0.7261 - val_loss: 0.3966 - val_mae: 0.5087\n",
      "Epoch 34/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8461 - mae: 0.7309 - val_loss: 0.4117 - val_mae: 0.5076\n",
      "Epoch 35/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8295 - mae: 0.7223 - val_loss: 0.4258 - val_mae: 0.5164\n",
      "Epoch 36/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8622 - mae: 0.7406 - val_loss: 0.3952 - val_mae: 0.5016\n",
      "Epoch 37/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8943 - mae: 0.7536 - val_loss: 0.3962 - val_mae: 0.5009\n",
      "Epoch 38/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7815 - mae: 0.7059 - val_loss: 0.3835 - val_mae: 0.4903\n",
      "Epoch 39/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7989 - mae: 0.7124 - val_loss: 0.3888 - val_mae: 0.4929\n",
      "Epoch 40/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8463 - mae: 0.7242 - val_loss: 0.3725 - val_mae: 0.4897\n",
      "Epoch 41/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7953 - mae: 0.7030 - val_loss: 0.3730 - val_mae: 0.4927\n",
      "Epoch 42/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7729 - mae: 0.6947 - val_loss: 0.3726 - val_mae: 0.4895\n",
      "Epoch 43/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7469 - mae: 0.6835 - val_loss: 0.3818 - val_mae: 0.4907\n",
      "Epoch 44/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8364 - mae: 0.7366 - val_loss: 0.3871 - val_mae: 0.5062\n",
      "Epoch 45/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7530 - mae: 0.6945 - val_loss: 0.3816 - val_mae: 0.4868\n",
      "Epoch 46/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8038 - mae: 0.6971 - val_loss: 0.3924 - val_mae: 0.4951\n",
      "Epoch 47/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7785 - mae: 0.6920 - val_loss: 0.3728 - val_mae: 0.4886\n",
      "Epoch 48/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7249 - mae: 0.6758 - val_loss: 0.3724 - val_mae: 0.4883\n",
      "Epoch 49/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8099 - mae: 0.7175 - val_loss: 0.4557 - val_mae: 0.5249\n",
      "Epoch 50/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8043 - mae: 0.7084 - val_loss: 0.3861 - val_mae: 0.4972\n"
     ]
    }
   ],
   "source": [
    "# Build the FNN model\n",
    "model_fnn = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='linear')  # Use 'linear' for regression tasks\n",
    "])\n",
    "\n",
    "model_fnn.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the FNN model\n",
    "history_fnn = model_fnn.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4257 - mae: 0.5177 \n",
      "CNN - Validation Loss: 0.4103565216064453, Validation MAE: 0.5169891119003296\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 0.4250 - mae: 0.5094\n",
      "FNN - Validation Loss: 0.3860984742641449, Validation MAE: 0.49722060561180115\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the CNN model\n",
    "loss_cnn, mae_cnn = model_cnn.evaluate(X_test_cnn, y_test)\n",
    "print(f\"CNN - Validation Loss: {loss_cnn}, Validation MAE: {mae_cnn}\")\n",
    "\n",
    "# Evaluate the FNN model\n",
    "loss_fnn, mae_fnn = model_fnn.evaluate(X_test, y_test)\n",
    "print(f\"FNN - Validation Loss: {loss_fnn}, Validation MAE: {mae_fnn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Câu b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 21.8836 - mae: 4.3583 - val_loss: 3.1416 - val_mae: 1.4411\n",
      "Epoch 2/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.0133 - mae: 1.5706 - val_loss: 2.4483 - val_mae: 1.3035\n",
      "Epoch 3/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6696 - mae: 1.5284 - val_loss: 1.8752 - val_mae: 1.1271\n",
      "Epoch 4/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9880 - mae: 1.3666 - val_loss: 1.4079 - val_mae: 0.9601\n",
      "Epoch 5/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1769 - mae: 1.1891 - val_loss: 1.0731 - val_mae: 0.8349\n",
      "Epoch 6/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7835 - mae: 1.0500 - val_loss: 1.0968 - val_mae: 0.8416\n",
      "Epoch 7/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8428 - mae: 1.0659 - val_loss: 0.9451 - val_mae: 0.7737\n",
      "Epoch 8/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5938 - mae: 1.0011 - val_loss: 1.0419 - val_mae: 0.8162\n",
      "Epoch 9/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3900 - mae: 0.9385 - val_loss: 1.1556 - val_mae: 0.8719\n",
      "Epoch 10/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4713 - mae: 0.9596 - val_loss: 0.8192 - val_mae: 0.7009\n",
      "Epoch 11/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3398 - mae: 0.9238 - val_loss: 1.0570 - val_mae: 0.8249\n",
      "Epoch 12/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3631 - mae: 0.9192 - val_loss: 1.0221 - val_mae: 0.7988\n",
      "Epoch 13/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2880 - mae: 0.8970 - val_loss: 0.6772 - val_mae: 0.6334\n",
      "Epoch 14/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2463 - mae: 0.8829 - val_loss: 1.2009 - val_mae: 0.9064\n",
      "Epoch 15/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2390 - mae: 0.8698 - val_loss: 0.8773 - val_mae: 0.7404\n",
      "Epoch 16/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2853 - mae: 0.8958 - val_loss: 0.6242 - val_mae: 0.6111\n",
      "Epoch 17/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2668 - mae: 0.8738 - val_loss: 0.7389 - val_mae: 0.6656\n",
      "Epoch 18/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1248 - mae: 0.8513 - val_loss: 0.6799 - val_mae: 0.6353\n",
      "Epoch 19/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0790 - mae: 0.8193 - val_loss: 0.6000 - val_mae: 0.6004\n",
      "Epoch 20/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2627 - mae: 0.9033 - val_loss: 0.9516 - val_mae: 0.7742\n",
      "Epoch 21/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1376 - mae: 0.8503 - val_loss: 0.9098 - val_mae: 0.7554\n",
      "Epoch 22/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.1313 - mae: 0.8384 - val_loss: 0.6211 - val_mae: 0.6110\n",
      "Epoch 23/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0763 - mae: 0.8180 - val_loss: 0.7044 - val_mae: 0.6480\n",
      "Epoch 24/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1411 - mae: 0.8294 - val_loss: 0.7590 - val_mae: 0.6724\n",
      "Epoch 25/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1272 - mae: 0.8393 - val_loss: 0.6598 - val_mae: 0.6258\n",
      "Epoch 26/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0149 - mae: 0.8024 - val_loss: 0.6876 - val_mae: 0.6404\n",
      "Epoch 27/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0017 - mae: 0.7959 - val_loss: 1.0001 - val_mae: 0.8022\n",
      "Epoch 28/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0667 - mae: 0.8233 - val_loss: 0.7164 - val_mae: 0.6581\n",
      "Epoch 29/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0499 - mae: 0.8045 - val_loss: 0.7893 - val_mae: 0.6904\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Build the CNN model with Dropout and Early Stopping\n",
    "model_cnn = Sequential([\n",
    "    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.25),\n",
    "    Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='linear')  # Use 'linear' for regression tasks\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Use Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the CNN model\n",
    "history_cnn = model_cnn.fit(X_train_cnn, y_train, epochs=50, validation_data=(X_test_cnn, y_test), \n",
    "                            callbacks=[early_stopping], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 25.0211 - mae: 4.7862 - val_loss: 5.6739 - val_mae: 1.9932\n",
      "Epoch 2/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.9939 - mae: 2.0241 - val_loss: 2.6067 - val_mae: 1.3025\n",
      "Epoch 3/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.6029 - mae: 1.7048 - val_loss: 2.2594 - val_mae: 1.2126\n",
      "Epoch 4/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.2273 - mae: 1.6218 - val_loss: 1.7973 - val_mae: 1.0543\n",
      "Epoch 5/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7586 - mae: 1.5380 - val_loss: 1.5830 - val_mae: 0.9762\n",
      "Epoch 6/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1409 - mae: 1.3754 - val_loss: 1.4518 - val_mae: 0.9407\n",
      "Epoch 7/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2327 - mae: 1.4184 - val_loss: 1.4736 - val_mae: 0.9530\n",
      "Epoch 8/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7944 - mae: 1.3379 - val_loss: 1.1636 - val_mae: 0.8272\n",
      "Epoch 9/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.2606 - mae: 1.1823 - val_loss: 1.1916 - val_mae: 0.8356\n",
      "Epoch 10/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3045 - mae: 1.1843 - val_loss: 1.1904 - val_mae: 0.8420\n",
      "Epoch 11/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3331 - mae: 1.1957 - val_loss: 1.0059 - val_mae: 0.7559\n",
      "Epoch 12/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1745 - mae: 1.1592 - val_loss: 0.9573 - val_mae: 0.7367\n",
      "Epoch 13/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.0256 - mae: 1.1221 - val_loss: 0.8115 - val_mae: 0.6750\n",
      "Epoch 14/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9144 - mae: 1.0650 - val_loss: 0.8345 - val_mae: 0.6854\n",
      "Epoch 15/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7602 - mae: 1.0278 - val_loss: 0.8030 - val_mae: 0.6707\n",
      "Epoch 16/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8300 - mae: 1.0471 - val_loss: 0.7575 - val_mae: 0.6469\n",
      "Epoch 17/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5537 - mae: 0.9762 - val_loss: 0.6870 - val_mae: 0.6093\n",
      "Epoch 18/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5967 - mae: 0.9799 - val_loss: 0.7050 - val_mae: 0.6160\n",
      "Epoch 19/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4423 - mae: 0.9401 - val_loss: 0.7610 - val_mae: 0.6442\n",
      "Epoch 20/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.5220 - mae: 0.9541 - val_loss: 0.6683 - val_mae: 0.5951\n",
      "Epoch 21/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3830 - mae: 0.9070 - val_loss: 0.6315 - val_mae: 0.5764\n",
      "Epoch 22/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3490 - mae: 0.8913 - val_loss: 0.7204 - val_mae: 0.6179\n",
      "Epoch 23/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2984 - mae: 0.8671 - val_loss: 0.6739 - val_mae: 0.5978\n",
      "Epoch 24/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2127 - mae: 0.8418 - val_loss: 0.6015 - val_mae: 0.5618\n",
      "Epoch 25/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3359 - mae: 0.8942 - val_loss: 0.6495 - val_mae: 0.5768\n",
      "Epoch 26/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2511 - mae: 0.8315 - val_loss: 0.6832 - val_mae: 0.5944\n",
      "Epoch 27/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1970 - mae: 0.8262 - val_loss: 0.5562 - val_mae: 0.5403\n",
      "Epoch 28/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1619 - mae: 0.8145 - val_loss: 0.6347 - val_mae: 0.5727\n",
      "Epoch 29/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1395 - mae: 0.8355 - val_loss: 0.6190 - val_mae: 0.5648\n",
      "Epoch 30/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0780 - mae: 0.7824 - val_loss: 0.5304 - val_mae: 0.5239\n",
      "Epoch 31/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1587 - mae: 0.8140 - val_loss: 0.5509 - val_mae: 0.5313\n",
      "Epoch 32/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0723 - mae: 0.7805 - val_loss: 0.5437 - val_mae: 0.5281\n",
      "Epoch 33/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0228 - mae: 0.7637 - val_loss: 0.5253 - val_mae: 0.5224\n",
      "Epoch 34/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.9891 - mae: 0.7568 - val_loss: 0.5196 - val_mae: 0.5199\n",
      "Epoch 35/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0683 - mae: 0.7788 - val_loss: 0.5621 - val_mae: 0.5365\n",
      "Epoch 36/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9528 - mae: 0.7492 - val_loss: 0.5303 - val_mae: 0.5276\n",
      "Epoch 37/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9812 - mae: 0.7475 - val_loss: 0.5131 - val_mae: 0.5243\n",
      "Epoch 38/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8963 - mae: 0.7120 - val_loss: 0.5403 - val_mae: 0.5278\n",
      "Epoch 39/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9822 - mae: 0.7479 - val_loss: 0.5131 - val_mae: 0.5173\n",
      "Epoch 40/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0800 - mae: 0.7715 - val_loss: 0.5147 - val_mae: 0.5219\n",
      "Epoch 41/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0358 - mae: 0.7775 - val_loss: 0.5277 - val_mae: 0.5236\n",
      "Epoch 42/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9023 - mae: 0.7149 - val_loss: 0.4790 - val_mae: 0.5050\n",
      "Epoch 43/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9659 - mae: 0.7485 - val_loss: 0.4795 - val_mae: 0.5152\n",
      "Epoch 44/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9176 - mae: 0.7309 - val_loss: 0.5119 - val_mae: 0.5242\n",
      "Epoch 45/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9964 - mae: 0.7561 - val_loss: 0.4944 - val_mae: 0.5132\n",
      "Epoch 46/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0333 - mae: 0.7670 - val_loss: 0.4806 - val_mae: 0.5110\n",
      "Epoch 47/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8862 - mae: 0.7030 - val_loss: 0.5120 - val_mae: 0.5125\n",
      "Epoch 48/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9308 - mae: 0.7308 - val_loss: 0.4805 - val_mae: 0.5112\n",
      "Epoch 49/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9158 - mae: 0.7385 - val_loss: 0.4792 - val_mae: 0.5100\n",
      "Epoch 50/50\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9466 - mae: 0.7502 - val_loss: 0.4637 - val_mae: 0.5054\n"
     ]
    }
   ],
   "source": [
    "# Build the FNN model with Regularization and Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model_fnn = Sequential([\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.001), input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='linear')  # Use 'linear' for regression tasks\n",
    "])\n",
    "\n",
    "model_fnn.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Use Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the FNN model\n",
    "history_fnn = model_fnn.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), \n",
    "                            callbacks=[early_stopping], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6136 - mae: 0.5969 \n",
      "CNN - Validation Loss: 0.5999828577041626, Validation MAE: 0.6003615260124207\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4851 - mae: 0.5115 \n",
      "FNN - Validation Loss: 0.4636557102203369, Validation MAE: 0.5054343938827515\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the CNN model\n",
    "loss_cnn, mae_cnn = model_cnn.evaluate(X_test_cnn, y_test)\n",
    "print(f\"CNN - Validation Loss: {loss_cnn}, Validation MAE: {mae_cnn}\")\n",
    "\n",
    "# Evaluate the FNN model\n",
    "loss_fnn, mae_fnn = model_fnn.evaluate(X_test, y_test)\n",
    "print(f\"FNN - Validation Loss: {loss_fnn}, Validation MAE: {mae_fnn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Câu c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Using RFE with Random Forest model\n",
    "# Initialize RFE with Random Forest Regressor as the estimator and select 8 features\n",
    "rfe = RFE(estimator=RandomForestRegressor(), n_features_to_select=8)\n",
    "\n",
    "# Fit RFE to the training data\n",
    "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "\n",
    "# Transform the test data using the selected features\n",
    "X_test_rfe = rfe.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikeras in c:\\python312\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\python312\\lib\\site-packages (from scikeras) (3.3.2)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in c:\\python312\\lib\\site-packages (from scikeras) (1.4.2)\n",
      "Requirement already satisfied: absl-py in c:\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.3.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\python312\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python312\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python312\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\python312\\lib\\site-packages (from optree->keras>=3.2.0->scikeras) (4.11.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\python312\\lib\\site-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\python312\\lib\\site-packages (from rich->keras>=3.2.0->scikeras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.007036756055458434 using {'epochs': 100, 'dropout': 0.5, 'batch_size': 128}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Define the function to create the model\n",
    "def create_model(dropout=0.0):\n",
    "    learning_rate = 0.01  # Set the learning rate here\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(X_train_rfe.shape[1],)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Use 'sigmoid' for binary classification\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model in a KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, dropout=0.0, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_dist = {\n",
    "    'dropout': [0.0, 0.2, 0.5],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [50, 100, 200]\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3, verbose=1, n_jobs=-1)\n",
    "random_search_result = random_search.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Print the best result\n",
    "print(f\"Best: {random_search_result.best_score_} using {random_search_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.0000e+00 - loss: -14.4510 - val_accuracy: 0.0000e+00 - val_loss: -104.3180\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: -179.7593 - val_accuracy: 0.0000e+00 - val_loss: -549.0411\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: -775.1617 - val_accuracy: 0.0000e+00 - val_loss: -1814.2795\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -2286.0251 - val_accuracy: 0.0000e+00 - val_loss: -4684.2979\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -5728.0986 - val_accuracy: 0.0000e+00 - val_loss: -10284.5400\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -12010.5303 - val_accuracy: 0.0000e+00 - val_loss: -19950.5117\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -22982.1777 - val_accuracy: 0.0000e+00 - val_loss: -35419.0078\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -38892.6836 - val_accuracy: 0.0000e+00 - val_loss: -58420.7734\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -63682.5664 - val_accuracy: 0.0000e+00 - val_loss: -91119.8359\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -95909.5391 - val_accuracy: 0.0000e+00 - val_loss: -135404.0938\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -143057.2656 - val_accuracy: 0.0000e+00 - val_loss: -193734.6562\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -203542.6719 - val_accuracy: 0.0000e+00 - val_loss: -268413.2500\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -275887.5000 - val_accuracy: 0.0000e+00 - val_loss: -361856.2500\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -372751.0938 - val_accuracy: 0.0000e+00 - val_loss: -476324.9062\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -491737.9062 - val_accuracy: 0.0000e+00 - val_loss: -615591.1875\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -616359.2500 - val_accuracy: 0.0000e+00 - val_loss: -780429.1250\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -774573.4375 - val_accuracy: 0.0000e+00 - val_loss: -974091.1875\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -966143.6250 - val_accuracy: 0.0000e+00 - val_loss: -1199027.2500\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -1178738.1250 - val_accuracy: 0.0000e+00 - val_loss: -1455491.2500\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -1469905.6250 - val_accuracy: 0.0000e+00 - val_loss: -1751554.7500\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -1755458.3750 - val_accuracy: 0.0000e+00 - val_loss: -2086362.0000\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -2064363.3750 - val_accuracy: 0.0000e+00 - val_loss: -2464303.7500\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -2486296.0000 - val_accuracy: 0.0000e+00 - val_loss: -2887511.5000\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -2813824.7500 - val_accuracy: 0.0000e+00 - val_loss: -3355028.0000\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: -3314874.5000 - val_accuracy: 0.0000e+00 - val_loss: -3873001.5000\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -3664491.0000 - val_accuracy: 0.0000e+00 - val_loss: -4436376.0000\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -4393003.0000 - val_accuracy: 0.0000e+00 - val_loss: -5063464.0000\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -5004401.0000 - val_accuracy: 0.0000e+00 - val_loss: -5749455.0000\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -5504107.5000 - val_accuracy: 0.0000e+00 - val_loss: -6491052.0000\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -6327905.5000 - val_accuracy: 0.0000e+00 - val_loss: -7295277.0000\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -7135915.5000 - val_accuracy: 0.0000e+00 - val_loss: -8164485.0000\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -7796143.5000 - val_accuracy: 0.0000e+00 - val_loss: -9094323.0000\n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -8987770.0000 - val_accuracy: 0.0000e+00 - val_loss: -10095646.0000\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -9938160.0000 - val_accuracy: 0.0000e+00 - val_loss: -11170883.0000\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -10927432.0000 - val_accuracy: 0.0000e+00 - val_loss: -12323794.0000\n",
      "Epoch 36/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -11985261.0000 - val_accuracy: 0.0000e+00 - val_loss: -13547136.0000\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -13175264.0000 - val_accuracy: 0.0000e+00 - val_loss: -14848803.0000\n",
      "Epoch 38/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -14565050.0000 - val_accuracy: 0.0000e+00 - val_loss: -16241941.0000\n",
      "Epoch 39/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: -16022833.0000 - val_accuracy: 0.0000e+00 - val_loss: -17721820.0000\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -17216716.0000 - val_accuracy: 0.0000e+00 - val_loss: -19266474.0000\n",
      "Epoch 41/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -18556376.0000 - val_accuracy: 0.0000e+00 - val_loss: -20916196.0000\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -21010386.0000 - val_accuracy: 0.0000e+00 - val_loss: -22672964.0000\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -21410384.0000 - val_accuracy: 0.0000e+00 - val_loss: -24492788.0000\n",
      "Epoch 44/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -23967404.0000 - val_accuracy: 0.0000e+00 - val_loss: -26434246.0000\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -26273698.0000 - val_accuracy: 0.0000e+00 - val_loss: -28463712.0000\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -28063308.0000 - val_accuracy: 0.0000e+00 - val_loss: -30604998.0000\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -29606802.0000 - val_accuracy: 0.0000e+00 - val_loss: -32840480.0000\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -32029810.0000 - val_accuracy: 0.0000e+00 - val_loss: -35191636.0000\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -34633124.0000 - val_accuracy: 0.0000e+00 - val_loss: -37643880.0000\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -36617936.0000 - val_accuracy: 0.0000e+00 - val_loss: -40200952.0000\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -39353908.0000 - val_accuracy: 0.0000e+00 - val_loss: -42875680.0000\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -40996192.0000 - val_accuracy: 0.0000e+00 - val_loss: -45650508.0000\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -44652116.0000 - val_accuracy: 0.0000e+00 - val_loss: -48562384.0000\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -47850972.0000 - val_accuracy: 0.0000e+00 - val_loss: -51590956.0000\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -49274160.0000 - val_accuracy: 0.0000e+00 - val_loss: -54722584.0000\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -54458876.0000 - val_accuracy: 0.0000e+00 - val_loss: -58019912.0000\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -56036824.0000 - val_accuracy: 0.0000e+00 - val_loss: -61418760.0000\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -59994612.0000 - val_accuracy: 0.0000e+00 - val_loss: -64960200.0000\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -62232860.0000 - val_accuracy: 0.0000e+00 - val_loss: -68622184.0000\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -66198880.0000 - val_accuracy: 0.0000e+00 - val_loss: -72425312.0000\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: -70301432.0000 - val_accuracy: 0.0000e+00 - val_loss: -76352272.0000\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -73230672.0000 - val_accuracy: 0.0000e+00 - val_loss: -80407120.0000\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -79342688.0000 - val_accuracy: 0.0000e+00 - val_loss: -84618648.0000\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: -82960688.0000 - val_accuracy: 0.0000e+00 - val_loss: -88984048.0000\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -84322432.0000 - val_accuracy: 0.0000e+00 - val_loss: -93424344.0000\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -87297928.0000 - val_accuracy: 0.0000e+00 - val_loss: -98023704.0000\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -94923800.0000 - val_accuracy: 0.0000e+00 - val_loss: -102814608.0000\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -101928256.0000 - val_accuracy: 0.0000e+00 - val_loss: -107815952.0000\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -106170304.0000 - val_accuracy: 0.0000e+00 - val_loss: -112950760.0000\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -106936096.0000 - val_accuracy: 0.0000e+00 - val_loss: -118193432.0000\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -113384000.0000 - val_accuracy: 0.0000e+00 - val_loss: -123611056.0000\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -117737888.0000 - val_accuracy: 0.0000e+00 - val_loss: -129178608.0000\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -126021696.0000 - val_accuracy: 0.0000e+00 - val_loss: -134941616.0000\n",
      "Epoch 74/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -130724664.0000 - val_accuracy: 0.0000e+00 - val_loss: -140836848.0000\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -136453312.0000 - val_accuracy: 0.0000e+00 - val_loss: -146861616.0000\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -143410448.0000 - val_accuracy: 0.0000e+00 - val_loss: -153043328.0000\n",
      "Epoch 77/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -143851248.0000 - val_accuracy: 0.0000e+00 - val_loss: -159352096.0000\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -154089968.0000 - val_accuracy: 0.0000e+00 - val_loss: -165877008.0000\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -155161408.0000 - val_accuracy: 0.0000e+00 - val_loss: -172522096.0000\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -169171536.0000 - val_accuracy: 0.0000e+00 - val_loss: -179444912.0000\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -174197136.0000 - val_accuracy: 0.0000e+00 - val_loss: -186507728.0000\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -179796976.0000 - val_accuracy: 0.0000e+00 - val_loss: -193728736.0000\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -185656560.0000 - val_accuracy: 0.0000e+00 - val_loss: -201078240.0000\n",
      "Epoch 84/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -193015328.0000 - val_accuracy: 0.0000e+00 - val_loss: -208592992.0000\n",
      "Epoch 85/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -199887344.0000 - val_accuracy: 0.0000e+00 - val_loss: -216306256.0000\n",
      "Epoch 86/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -208257648.0000 - val_accuracy: 0.0000e+00 - val_loss: -224213472.0000\n",
      "Epoch 87/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -213538368.0000 - val_accuracy: 0.0000e+00 - val_loss: -232259168.0000\n",
      "Epoch 88/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -222079872.0000 - val_accuracy: 0.0000e+00 - val_loss: -240495824.0000\n",
      "Epoch 89/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -234488720.0000 - val_accuracy: 0.0000e+00 - val_loss: -248947248.0000\n",
      "Epoch 90/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -238906896.0000 - val_accuracy: 0.0000e+00 - val_loss: -257568640.0000\n",
      "Epoch 91/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -252779728.0000 - val_accuracy: 0.0000e+00 - val_loss: -266462256.0000\n",
      "Epoch 92/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -252192784.0000 - val_accuracy: 0.0000e+00 - val_loss: -275469888.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -266916256.0000 - val_accuracy: 0.0000e+00 - val_loss: -284732704.0000\n",
      "Epoch 94/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: -275654208.0000 - val_accuracy: 0.0000e+00 - val_loss: -294194336.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -277836608.0000 - val_accuracy: 0.0000e+00 - val_loss: -303737280.0000\n",
      "Epoch 96/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -286835424.0000 - val_accuracy: 0.0000e+00 - val_loss: -313511168.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -298610272.0000 - val_accuracy: 0.0000e+00 - val_loss: -323441344.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -306479872.0000 - val_accuracy: 0.0000e+00 - val_loss: -333599104.0000\n",
      "Epoch 99/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -313755136.0000 - val_accuracy: 0.0000e+00 - val_loss: -343931904.0000\n",
      "Epoch 100/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -331596160.0000 - val_accuracy: 0.0000e+00 - val_loss: -354552896.0000\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: -350260288.0000 \n",
      "Optimized FNN - Loss: -354552896.0, Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Using the best parameters from RandomizedSearchCV\n",
    "best_params = random_search_result.best_params_\n",
    "\n",
    "# Create an FNN model with the best hyperparameters\n",
    "def create_model(dropout_rate=0.0, learning_rate=0.01, batch_size=32, epochs=100):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(X_train_rfe.shape[1],)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_fnn_optimized = create_model(\n",
    "    dropout_rate=best_params['dropout'],\n",
    "    learning_rate=0.01,  # Use a fixed learning rate\n",
    "    batch_size=best_params['batch_size'],\n",
    "    epochs=best_params['epochs']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history_fnn_optimized = model_fnn_optimized.fit(X_train_rfe, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], validation_data=(X_test_rfe, y_test), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss_fnn_opt, acc_fnn_opt = model_fnn_optimized.evaluate(X_test_rfe, y_test)\n",
    "print(f\"Optimized FNN - Loss: {loss_fnn_opt}, Accuracy: {acc_fnn_opt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 0.4851 - mae: 0.5115\n",
      "Initial FNN - Loss: 0.4636557102203369, MAE: 0.5054343938827515\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - accuracy: 0.0000e+00 - loss: -350260288.0000\n",
      "Optimized FNN - Loss: -354552896.0, MAE: 0.0\n",
      "Improvement in MAE: 0.5054343938827515\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the initial FNN model\n",
    "loss_fnn, mae_fnn = model_fnn.evaluate(X_test, y_test)\n",
    "print(f\"Initial FNN - Loss: {loss_fnn}, MAE: {mae_fnn}\")\n",
    "\n",
    "# Evaluate the optimized FNN model\n",
    "loss_fnn_opt, mae_fnn_opt = model_fnn_optimized.evaluate(X_test_rfe, y_test)\n",
    "print(f\"Optimized FNN - Loss: {loss_fnn_opt}, MAE: {mae_fnn_opt}\")\n",
    "\n",
    "# Compare performance\n",
    "print(f\"Improvement in MAE: {mae_fnn - mae_fnn_opt}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
