{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "iris = pd.read_csv('iris.data')\n",
    "X = iris.values[:, :-1]\n",
    "y = iris.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define neural network architecture with each optimizer\n",
    "model_gd = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model_sgd = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model_mini_batch = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model_momentum = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model_rmsprop = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model_adam = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model_adagrad = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model_nadam = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model_amsgrad = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "model_adamax = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile models with respective optimizers\n",
    "model_gd.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_sgd.compile(optimizer=tf.keras.optimizers.SGD(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_mini_batch.compile(optimizer=tf.keras.optimizers.SGD(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_momentum.compile(optimizer=tf.keras.optimizers.SGD(momentum=0.9), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_rmsprop.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_adam.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_adagrad.compile(optimizer='adagrad', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_nadam.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_amsgrad.compile(optimizer=tf.keras.optimizers.Adam(amsgrad=True), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_adamax.compile(optimizer='adamax', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "history_gd = model_gd.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), verbose=0)\n",
    "history_sgd = model_sgd.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), verbose=0)\n",
    "history_mini_batch = model_mini_batch.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), verbose=0)\n",
    "history_momentum = model_momentum.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), verbose=0)\n",
    "history_rmsprop = model_rmsprop.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), verbose=0)\n",
    "history_adam = model_adam.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), verbose=0)\n",
    "history_adagrad = model_adagrad.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), verbose=0)\n",
    "history_nadam = model_nadam.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), verbose=0)\n",
    "history_amsgrad = model_amsgrad.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), verbose=0)\n",
    "history_adamax = model_adamax.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8333 - loss: 0.3734\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8333 - loss: 0.3516\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8667 - loss: 0.3445\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9000 - loss: 0.1627\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8667 - loss: 0.2981\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8333 - loss: 0.3727\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5000 - loss: 0.9156\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8333 - loss: 0.3249\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8667 - loss: 0.2993\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8333 - loss: 0.3805\n",
      "\n",
      "Gradient Descent (GD) Optimizer:\n",
      "Loss: 0.3734489381313324, Accuracy: 0.8333333134651184\n",
      "\n",
      "Stochastic Gradient Descent (SGD) Optimizer:\n",
      "Loss: 0.3516466021537781, Accuracy: 0.8333333134651184\n",
      "\n",
      "Mini-batch Gradient Descent Optimizer:\n",
      "Loss: 0.3445341885089874, Accuracy: 0.8666666746139526\n",
      "\n",
      "Momentum Optimizer:\n",
      "Loss: 0.1626550853252411, Accuracy: 0.8999999761581421\n",
      "\n",
      "RMSProp Optimizer:\n",
      "Loss: 0.29807206988334656, Accuracy: 0.8666666746139526\n",
      "\n",
      "Adam Optimizer:\n",
      "Loss: 0.3727005422115326, Accuracy: 0.8333333134651184\n",
      "\n",
      "Adagrad Optimizer:\n",
      "Loss: 0.9156109690666199, Accuracy: 0.5\n",
      "\n",
      "Nadam Optimizer:\n",
      "Loss: 0.32490822672843933, Accuracy: 0.8333333134651184\n",
      "\n",
      "AMSGrad Optimizer:\n",
      "Loss: 0.29931360483169556, Accuracy: 0.8666666746139526\n",
      "\n",
      "Adamax Optimizer:\n",
      "Loss: 0.38051337003707886, Accuracy: 0.8333333134651184\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "loss_gd, accuracy_gd = model_gd.evaluate(X_test, y_test)\n",
    "loss_sgd, accuracy_sgd = model_sgd.evaluate(X_test, y_test)\n",
    "loss_mini_batch, accuracy_mini_batch = model_mini_batch.evaluate(X_test, y_test)\n",
    "loss_momentum, accuracy_momentum = model_momentum.evaluate(X_test, y_test)\n",
    "loss_rmsprop, accuracy_rmsprop = model_rmsprop.evaluate(X_test, y_test)\n",
    "loss_adam, accuracy_adam = model_adam.evaluate(X_test, y_test)\n",
    "loss_adagrad, accuracy_adagrad = model_adagrad.evaluate(X_test, y_test)\n",
    "loss_nadam, accuracy_nadam = model_nadam.evaluate(X_test, y_test)\n",
    "loss_amsgrad, accuracy_amsgrad = model_amsgrad.evaluate(X_test, y_test)\n",
    "loss_adamax, accuracy_adamax = model_adamax.evaluate(X_test, y_test)\n",
    "\n",
    "# In kết quả đánh giá\n",
    "print(\"\\nGradient Descent (GD) Optimizer:\")\n",
    "print(f\"Loss: {loss_gd}, Accuracy: {accuracy_gd}\")\n",
    "print(\"\\nStochastic Gradient Descent (SGD) Optimizer:\")\n",
    "print(f\"Loss: {loss_sgd}, Accuracy: {accuracy_sgd}\")\n",
    "print(\"\\nMini-batch Gradient Descent Optimizer:\")\n",
    "print(f\"Loss: {loss_mini_batch}, Accuracy: {accuracy_mini_batch}\")\n",
    "print(\"\\nMomentum Optimizer:\")\n",
    "print(f\"Loss: {loss_momentum}, Accuracy: {accuracy_momentum}\")\n",
    "print(\"\\nRMSProp Optimizer:\")\n",
    "print(f\"Loss: {loss_rmsprop}, Accuracy: {accuracy_rmsprop}\")\n",
    "print(\"\\nAdam Optimizer:\")\n",
    "print(f\"Loss: {loss_adam}, Accuracy: {accuracy_adam}\")\n",
    "print(\"\\nAdagrad Optimizer:\")\n",
    "print(f\"Loss: {loss_adagrad}, Accuracy: {accuracy_adagrad}\")\n",
    "print(\"\\nNadam Optimizer:\")\n",
    "print(f\"Loss: {loss_nadam}, Accuracy: {accuracy_nadam}\")\n",
    "print(\"\\nAMSGrad Optimizer:\")\n",
    "print(f\"Loss: {loss_amsgrad}, Accuracy: {accuracy_amsgrad}\")\n",
    "print(\"\\nAdamax Optimizer:\")\n",
    "print(f\"Loss: {loss_adamax}, Accuracy: {accuracy_adamax}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
