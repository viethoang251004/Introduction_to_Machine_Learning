# -*- coding: utf-8 -*-
"""Midterm_ML

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jo16meIXg3WIr5N_Kx5oZuyrR7qtqdUM

Classification
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report
from sklearn.preprocessing import MinMaxScaler
from warnings import filterwarnings
from sklearn.metrics import accuracy_score,confusion_matrix
filterwarnings(action='ignore')

"""Load data"""

wine = pd.read_csv("winequality-red.csv")
wine.sample(15)

wine.info()

wine.describe()

wine.isnull().sum()

wine.groupby('quality').mean()

"""Data visualization"""

wine.hist(figsize=(10,10),bins=50)
plt.show()

"""Data normalization"""

wine['quality'].unique()

wine['goodquality'] = [1 if x >=7 else 0 for x in wine['quality']]
wine.sample(5)

df = wine.values
X = wine.values[:,:-1]
Y = wine.values[:,-1]

scaler = MinMaxScaler()
scaler.fit(X)
X_scaled = scaler.fit_transform(X)
print(df)

"""Data preparing for Machine learning modeling"""

X_train, X_test, Y_train, Y_test = train_test_split(X,Y,random_state=7, test_size=0.2)

model_res=pd.DataFrame(columns=['Model','Score'])

"""Training"""

def deploy_logistic_classifier(model, X_test):
    y_pred = model.predict(X_test)
    model_res.loc[len(model_res)] = ['LogisticRegression', accuracy_score(Y_test,y_pred)]
    print("Predict (Logistic Classifier):", y_pred)
    print(classification_report(Y_test, y_pred))

def deploy_svm_classifier(model, X_test):
    y_pred = model.predict(X_test)
    model_res.loc[len(model_res)] = ['SVM Classifier', accuracy_score(Y_test,y_pred)]
    print("Predict (SVM Classifier):", y_pred)
    print(classification_report(Y_test, y_pred))

def deploy_neural_network_classifier(model, X_test):
    y_pred = model.predict(X_test)
    model_res.loc[len(model_res)] = ['Neural Network Classifier', accuracy_score(Y_test,y_pred)]
    print("Predict (Neural Network Classifier):", y_pred)
    print(classification_report(Y_test, y_pred))

"""Prediction and Evaluation"""

model_logistic = LogisticRegression()
model_logistic.fit(X_train, Y_train)
deploy_logistic_classifier(model_logistic, X_test)
model_res

model_svm = SVC()
model_svm.fit(X_train, Y_train)
deploy_svm_classifier(model_svm, X_test)
model_res

model_neural_network = MLPClassifier()
model_neural_network.fit(X_train, Y_train)
deploy_neural_network_classifier(model_neural_network, X_test)
model_res

model_res = model_res.sort_values(by='Score',ascending=False)
model_res

"""Overfitting regression"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Load the dataset
data = pd.read_csv("winequality-red.csv")

# Assume you have a single feature named 'quality' and a target variable named 'goodquality'
X = wine[['quality']].values
y = wine['goodquality'].values

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit polynomial regression models with different degrees
degrees = [1, 3, 4, 5, 7, 9, 11, 13, 15]
plt.figure(figsize=(18, 12))

for i in range(len(degrees)):
    ax = plt.subplot(3, 3, i + 1)
    plt.setp(ax, xticks=(), yticks=())

    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)
    linear_regression = LinearRegression()

    # Create a pipeline
    pipeline = Pipeline([("polynomial_features", polynomial_features),
                         ("linear_regression", linear_regression)])
    pipeline.fit(X_train, y_train)

    # Evaluate the model on the training set
    y_train_pred = pipeline.predict(X_train)
    mse_train = mean_squared_error(y_train, y_train_pred)

    # Evaluate the model on the test set
    y_test_pred = pipeline.predict(X_test)
    mse_test = mean_squared_error(y_test, y_test_pred)

    # Plot the training data points
    plt.scatter(X_train, y_train, color='blue', alpha=0.5, label="Train data")

    # Plot the test data points
    plt.scatter(X_test, y_test, color='red', alpha=0.5, label="Test data")

    # Plot the model's predictions
    x_range = np.linspace(X.min(), X.max(), 100)
    plt.plot(x_range, pipeline.predict(x_range.reshape(-1, 1)), color='green',
             label="Fit (degree %d)\nTrain MSE = %.2f\nTest MSE = %.2f" % (degrees[i], mse_train, mse_test))
    plt.xlabel("Quality")
    plt.ylabel("Good Quality")
    plt.legend(loc="best")
    plt.title("Degree %d" % degrees[i])

    # Highlight overfitting
    if mse_train < mse_test:
        plt.title("Degree %d (Overfitting)" % degrees[i])

plt.tight_layout()
plt.show()

"""Overfitting Classification"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix


# Assume you have a single feature named 'quality' and a target variable named 'goodquality' (binary)
X = wine[['quality']].values
y = wine['goodquality'].values

# Convert the target variable to binary labels
y = np.where(y > 5, 1, 0)

# Split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit logistic regression models with different degrees
degrees = [1, 3, 4, 5, 7, 9, 11, 13, 15]
plt.figure(figsize=(18, 12))

for i in range(len(degrees)):
    ax = plt.subplot(3, 3, i + 1)
    plt.setp(ax, xticks=(), yticks=())

    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)
    logistic_regression = LogisticRegression()

    # Create a pipeline
    pipeline = Pipeline([("polynomial_features", polynomial_features),
                         ("logistic_regression", logistic_regression)])
    pipeline.fit(X_train, y_train)

    # Evaluate the model on the training set
    y_train_pred = pipeline.predict(X_train)
    train_accuracy = accuracy_score(y_train, y_train_pred)
    train_confusion_matrix = confusion_matrix(y_train, y_train_pred)

    # Evaluate the model on the test set
    y_test_pred = pipeline.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_test_pred)
    test_confusion_matrix = confusion_matrix(y_test, y_test_pred)

    # Plot the training data points
    plt.scatter(X_train, y_train, color='blue', alpha=0.5, label="Train data")

    # Plot the test data points
    plt.scatter(X_test, y_test, color='red', alpha=0.5, label="Test data")

    # Plot the model's predictions
    x_range = np.linspace(X.min(), X.max(), 100)
    plt.plot(x_range, pipeline.predict(x_range.reshape(-1, 1)), color='green',
             label="Fit (degree %d)\nTrain Accuracy = %.2f\nTest Accuracy = %.2f" % (degrees[i], train_accuracy, test_accuracy))
    plt.xlabel("Quality")
    plt.ylabel("Good Quality (Binary)")
    plt.legend(loc="best")
    plt.title("Degree %d" % degrees[i])

    # Highlight overfitting
    if train_accuracy > test_accuracy:
        plt.title("Degree %d (Overfitting)" % degrees[i])

plt.tight_layout()
plt.show()