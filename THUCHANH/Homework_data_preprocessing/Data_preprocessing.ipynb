{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning:\n",
        "1. Handling missing values: To impute missing values, you can use statistical measures such as mean, median, or mode to fill in the missing values. Alternatively, you can remove instances with missing values if they are not significant or imputation is not appropriate for your dataset. Here's an example of imputing missing values using the mean:\n",
        "(1. Xử lý giá trị thiếu: Để điền vào giá trị thiếu, bạn có thể sử dụng các phương pháp thống kê như trung bình, trung vị, hoặc mode để điền vào các giá trị thiếu. Hoặc bạn có thể loại bỏ các mẫu dữ liệu có giá trị thiếu nếu chúng không quan trọng hoặc việc điền giá trị không phù hợp cho tập dữ liệu của bạn. Dưới đây là một ví dụ về việc điền giá trị thiếu bằng giá trị trung bình:)"
      ],
      "metadata": {
        "id": "tSHD43kQg6Bj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLwBdxD8f_Hz"
      },
      "outputs": [],
      "source": [
        "# Impute missing values with the mean\n",
        "df.fillna(df.mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Handling outliers: Outliers can be detected using various statistical methods such as the Z-score or the interquartile range (IQR). Once identified, you can choose to remove the outliers or transform them using techniques like winsorization. Here's an example of removing outliers using the Z-score:(2. Xử lý các giá trị ngoại lệ: Các giá trị ngoại lệ có thể được phát hiện bằng các phương pháp thống kê như Z-score hoặc phạm vi từ quartile (IQR). Sau khi xác định, bạn có thể chọn loại bỏ các giá trị ngoại lệ hoặc biến đổi chúng bằng các kỹ thuật như winsorization. Dưới đây là một ví dụ về việc loại bỏ các giá trị ngoại lệ bằng Z-score:)"
      ],
      "metadata": {
        "id": "-iis9NbahRKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "z_scores = stats.zscore(df['numeric_column'])\n",
        "threshold = 3\n",
        "\n",
        "# Remove outliers\n",
        "df = df[(z_scores < threshold)]"
      ],
      "metadata": {
        "id": "fptMiLxPhWcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Noise reduction: To smooth noisy data, you can apply techniques like binning or filtering. Binning involves dividing the data into bins and replacing the values with the bin averages. Filtering can be done using techniques like moving averages or median filtering. Here's an example of applying a moving average filter:(3. Giảm nhiễu: Để làm mịn dữ liệu nhiễu, bạn có thể áp dụng các kỹ thuật như chia thành bin hoặc lọc. Chia thành bin liên quan đến việc chia dữ liệu thành các bin và thay thế các giá trị bằng giá trị trung bình của bin. Lọc có thể được thực hiện bằng cách sử dụng các kỹ thuật như trung bình động hoặc lọc trung vị. Dưới đây là một ví dụ về việc áp dụng bộ lọc trung bình động:)"
      ],
      "metadata": {
        "id": "1J4tMSZmhZQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 3\n",
        "df['smoothed_column'] = df['original_column'].rolling(window=window_size).mean()"
      ],
      "metadata": {
        "id": "GmHpWbUEhcOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Integration:\n",
        "\n",
        "1. Combining data from multiple sources: Use pandas' merge or concat functions to combine dataframes based on common columns or indices.(1. Kết hợp dữ liệu từ nhiều nguồn: Sử dụng các hàm merge hoặc concat trong thư viện pandas để kết hợp các khung dữ liệu dựa trên cột hoặc chỉ số chung.)"
      ],
      "metadata": {
        "id": "8kLqJ3Cxhg3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(df1, df2, on='common_column')"
      ],
      "metadata": {
        "id": "LDCIlhIZhjjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Handling inconsistencies: Standardize naming conventions and data formats to ensure consistency across different sources. You can use string manipulation functions to clean and transform data. For example, you can use the str.lower() function to convert strings to lowercase.(2. Xử lý không nhất quán: Chuẩn hóa quy tắc đặt tên và định dạng dữ liệu để đảm bảo tính nhất quán giữa các nguồn khác nhau. Bạn có thể sử dụng các hàm xử lý chuỗi để làm sạch và biến đổi dữ liệu. Ví dụ, bạn có thể sử dụng hàm str.lower() để chuyển đổi chuỗi thành chữ thường.)"
      ],
      "metadata": {
        "id": "u2jTb3iuhlGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Transformation:\n",
        "\n",
        "1. Normalization: Scale numerical features to a similar range, such as [0, 1], using techniques like Min-Max scaling.(1. Chuẩn hóa: Chỉnh tỷ lệ các đặc trưng số thành khoảng tương tự, như [0, 1], bằng cách sử dụng các kỹ thuật như Min-Max scaling.)"
      ],
      "metadata": {
        "id": "JjEjzIUGh0EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df['normalized_column'] = scaler.fit_transform(df[['numeric_column']])"
      ],
      "metadata": {
        "id": "OneKqy8Zh3rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Standardization: Transform data to have a mean of 0 and a standard deviation of 1 using techniques like z-score normalization."
      ],
      "metadata": {
        "id": "SqSSeLKvh4O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df['standardized_column'] = scaler.fit_transform(df[['numeric_column']])"
      ],
      "metadata": {
        "id": "khEbH8ZAh9dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Encoding categorical variables: Convert categorical variables into numerical representations suitable for machine learning algorithms. One-hot encoding creates binary columns for each category, while label encoding assigns a unique numerical label to each category."
      ],
      "metadata": {
        "id": "SZXUDxoYh-9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding\n",
        "df_encoded = pd.get_dummies(df, columns=['categorical_column'])\n",
        "\n",
        "# Label encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "df['encoded_column'] = encoder.fit_transform(df['categorical_column'])"
      ],
      "metadata": {
        "id": "OqjalwroiBXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection:\n",
        "\n",
        "1. Identify relevant features: Use techniques like correlation analysis, feature importance ranking, or domain knowledge to select features that contribute the most to the prediction task.\n",
        "\n",
        "2. Remove redundant or irrelevant features: Remove features that have low variance, high correlation with other features, or do not provide meaningful information for the task at hand."
      ],
      "metadata": {
        "id": "4zxist7riDRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering:\n",
        "\n",
        "1. Create new features: Generate new features based on existing ones that might better represent patterns in the data. This can include polynomial features, interaction features, or domain-specific feature engineering."
      ],
      "metadata": {
        "id": "yZy4WRjiiHzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['new_feature'] = df['feature1'] * df['feature2']  # Interaction feature\n",
        "df['squared_feature'] = df['feature'] ** 2  # Polynomial feature"
      ],
      "metadata": {
        "id": "FIaO6T7hiLAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Reduction:\n",
        "\n",
        "1. Dimensionality reduction: Apply techniques like Principal Component Analysis (PCA) or Singular Value Decomposition (SVD) to reduce the number of features while preserving important information."
      ],
      "metadata": {
        "id": "C7WbhEsGiPET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced_features = pca.fit_transform(df)"
      ],
      "metadata": {
        "id": "Zr6Fqg_CiRl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Splitting:\n",
        "\n",
        "1. Splitting into training, validation, and test sets: Use functions like train_test_split from scikit-learn to split the dataset into training, validation, and test sets."
      ],
      "metadata": {
        "id": "KAa2pz_BiT-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "T4XIfDHxiW-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Cross-validation: Perform k-fold cross-validation to evaluate model performance across multiple train-test splits."
      ],
      "metadata": {
        "id": "LJAu4Qz2iZY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(model, X, y, cv=5)"
      ],
      "metadata": {
        "id": "dZSWcMxBiaqg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}